{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "998080d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms, models # --> new\n",
    "from torchmetrics.functional import accuracy\n",
    "# from pytorch_lightning.metrics.functional import accuracy\n",
    "from torch.utils.data import DataLoader, random_split \n",
    "import torchxrayvision as xrv\n",
    "\n",
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ecf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_imgnumpy(image: torch.Tensor, denormalize=False) -> np.ndarray:\n",
    "    assert image.dim() == 3, f\"expecting [3,256,256], the input size is {image.size()}\" \n",
    "    \n",
    "    imgnumpy = image.numpy().transpose(1,2,0)\n",
    "    if denormalize:\n",
    "        imgnumpy = imgnumpy*np.array((0.485, 0.456, 0.406)) + np.array((0.229, 0.224, 0.22))\n",
    "    \n",
    "    imgnumpy = imgnumpy.clip(0, 1)\n",
    "    return imgnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1a837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_imgnumpy_simple(image):\n",
    "    imgnumpy = image\n",
    "    imgnumpy = imgnumpy.squeeze()\n",
    "    return imgnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc9a918b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XRV-ResNet-resnet50-res512-all\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# from cifar10_models.inception import inception_v3\n",
    "# from cifar10_models.googlenet import googlenet\n",
    "# from cifar10_models.mobilenetv2 import mobilenet_v2\n",
    "# from cifar10_models.resnet import resnet18\n",
    "# from cifar10_models.densenet import densenet121\n",
    "modelUsed = xrv.models.ResNet(weights=\"resnet50-res512-all\")\n",
    "modelUsed.eval()\n",
    "\n",
    "print(modelUsed)\n",
    "print(modelUsed.model.fc.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce8b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNormalize(torch.nn.Module):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    This transform does not support PIL Image.\n",
    "    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\n",
    "    channels, this transform will normalize each channel of the input\n",
    "    ``torch.*Tensor`` i.e.,\n",
    "    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "    .. note::\n",
    "        This transform acts out of place, i.e., it does not mutate the input tensor.\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "        inplace(bool,optional): Bool to make this operation in-place.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return (2 * (img.astype(np.float32) / 255) - 1.) * 1024\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "class ToNumpy(torch.nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, img):\n",
    "        return np.array(img)\n",
    "    \n",
    "class AddColorChannel(torch.nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Check that images are 2D arrays\n",
    "        if len(img.shape) > 2:\n",
    "            img = img[:, :, 0]\n",
    "        if len(img.shape) < 2:\n",
    "            print(\"error, dimension lower than 2 for image\")\n",
    "        return img[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f204b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "warning_log = {}\n",
    "\n",
    "def fix_resolution(x, resolution: int, model: nn.Module):\n",
    "        \"\"\"Check resolution of input and resize to match requested.\"\"\"\n",
    "\n",
    "        # just skip it if upsample was removed somehow\n",
    "        if not hasattr(model, 'upsample') or (model.upsample == None):\n",
    "            return x\n",
    "\n",
    "        if (x.shape[2] != resolution) | (x.shape[3] != resolution):\n",
    "            if not hash(model) in warning_log:\n",
    "                print(\"Warning: Input size ({}x{}) is not the native resolution ({}x{}) for this model. A resize will be performed but this could impact performance.\".format(x.shape[2], x.shape[3], resolution, resolution))\n",
    "                warning_log[hash(model)] = True\n",
    "            return model.upsample(x)\n",
    "        return x\n",
    "\n",
    "def warn_normalization(x):\n",
    "    \"\"\"Check normalization of input and warn if possibly wrong. When \n",
    "    processing an image that may likely not have the correct \n",
    "    normalization we can issue a warning. But running min and max on \n",
    "    every image/batch is costly so we only do it on the first image/batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Only run this check on the first image so we don't hurt performance.\n",
    "    if not \"norm_check\" in warning_log:\n",
    "        x_min = x.min()\n",
    "        x_max = x.max()\n",
    "        if torch.logical_or(-255 < x_min, x_max < 255) or torch.logical_or(x_min < -1024, 1024 < x_max):\n",
    "            print(f'Warning: Input image does not appear to be normalized correctly. The input image has the range [{x_min:.2f},{x_max:.2f}] which doesn\\'t seem to be in the [-1024,1024] range. This warning may be wrong though. Only the first image is tested and we are only using a heuristic in an attempt to save a user from using the wrong normalization.')\n",
    "            warning_log[\"norm_correct\"] = False\n",
    "        else:\n",
    "            warning_log[\"norm_correct\"] = True\n",
    "              \n",
    "        warning_log[\"norm_check\"] = True\n",
    "    \n",
    "class FinetunedModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # load pretrained model\n",
    "        model = xrv.models.ResNet(weights=\"resnet50-res512-all\")\n",
    "        \n",
    "        self.conv1 = model.model.conv1\n",
    "        self.bn1 = model.model.bn1\n",
    "        self.relu = model.model.relu\n",
    "        self.maxpool = model.model.maxpool\n",
    "\n",
    "        self.layer1 = model.model.layer1\n",
    "        self.layer2 = model.model.layer2\n",
    "        self.layer3 = model.model.layer3\n",
    "        self.layer4 = model.model.layer4\n",
    "\n",
    "        self.avgpool = model.model.avgpool\n",
    "        \n",
    "        self.fc = model.model.fc\n",
    "        \n",
    "        self.model = model.model\n",
    "        \n",
    "#         freeze the feature learning\n",
    "        for param in self.conv1.parameters():\n",
    "              param.requires_grad = False\n",
    "        \n",
    "        for param in self.bn1.parameters():\n",
    "              param.requires_grad = False\n",
    "                \n",
    "        for param in self.relu.parameters():\n",
    "              param.requires_grad = False\n",
    "                \n",
    "        for param in self.maxpool.parameters():\n",
    "              param.requires_grad = False\n",
    "                \n",
    "        for param in self.layer1.parameters():\n",
    "              param.requires_grad = False\n",
    "        \n",
    "        for param in self.layer2.parameters():\n",
    "              param.requires_grad = False\n",
    "                \n",
    "        for param in self.layer3.parameters():\n",
    "              param.requires_grad = False\n",
    "                \n",
    "        for param in self.layer4.parameters():\n",
    "              param.requires_grad = False\n",
    "        \n",
    "        # change the number of output classes of the last layer\n",
    "        # this is useless line as it the number of output classes is already set to be 10\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=self.fc.in_features,\n",
    "            out_features=2)\n",
    "        \n",
    "        # follow https://pytorch.org/hub/pytorch_vision_alexnet/\n",
    "        tf_tonumpy = ToNumpy()\n",
    "        tf_custom_normalize = CustomNormalize()\n",
    "        tf_add_color_channel = AddColorChannel()\n",
    "        tf_totensor = transforms.ToTensor()\n",
    "        self.tf_compose = transforms.Compose([\n",
    "            tf_tonumpy,\n",
    "            tf_custom_normalize,\n",
    "            tf_add_color_channel,\n",
    "#             xrv.datasets.XRayCenterCrop(),\n",
    "            xrv.datasets.XRayResizer(512),\n",
    "#             tf_totensor\n",
    "        ])\n",
    "    \n",
    "    def features(self, x):\n",
    "        x = fix_resolution(x, 512, self)\n",
    "        warn_normalization(x)\n",
    "        \n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        x = self.model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = fix_resolution(x, 512, self)\n",
    "        warn_normalization(x)\n",
    "        \n",
    "        out = self.model(x)\n",
    "        \n",
    "        if hasattr(self, 'apply_sigmoid') and self.apply_sigmoid:\n",
    "            out = torch.sigmoid(out)\n",
    "        \n",
    "        if hasattr(self,\"op_threshs\") and (self.op_threshs != None):\n",
    "            out = torch.sigmoid(out)\n",
    "            out = op_norm(out, self.op_threshs)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Copy paste from the previous article\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        outputs = self.forward(inputs)\n",
    "        loss = F.cross_entropy(outputs,labels) # --> NEW. Using nn.CrossEntropyLoss\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # This is new, but the structure is the same as training_step\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        outputs = self.forward(inputs)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        loss = F.cross_entropy(outputs,labels)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = accuracy(preds, labels) # --> NEW\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # This is new, but the structure is the same as test_step\n",
    "        # but I replace val_loss --> test_loss etc\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        outputs = self.forward(inputs)\n",
    "        loss = F.cross_entropy(outputs,labels)\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = accuracy(preds, labels)\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # split, transform, secretly move to GPU (if needed) by PL (not by us)\n",
    "        if stage == 'fit' or stage is None:\n",
    "            dataset_full = datasets.ImageFolder(root='./data/Batch 3/Train/', transform=self.tf_compose)\n",
    "            \n",
    "            # split\n",
    "            SIZE_TRAIN_DATA = int(len(dataset_full)*0.75)\n",
    "            SIZE_VAL_DATA = len(dataset_full)-SIZE_TRAIN_DATA\n",
    "            self.dataset_train, self.dataset_val = random_split(dataset_full, [SIZE_TRAIN_DATA,SIZE_VAL_DATA])\n",
    "            \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.dataset_test = datasets.ImageFolder(root='./data//Batch 3/Test/', transform=self.tf_compose)\n",
    "            \n",
    "#         import pdb; pdb.set_trace()\n",
    "            \n",
    "    def train_dataloader(self): \n",
    "        return DataLoader(self.dataset_train, batch_size=50, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val, batch_size=50, num_workers=0)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, batch_size=50, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3580ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.seed_everything(88) # --> for consistency, change the number with your favorite number :D\n",
    "\n",
    "# model = FinetunedModel()\n",
    "\n",
    "# # most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# try:\n",
    "#     trainer = pl.Trainer(gpus=1,max_epochs=1,default_root_dir='./batch3_logs')\n",
    "# except Exception as e:\n",
    "#     # most likely due to GPU, so fallback to non GPU\n",
    "#     print(e)\n",
    "#     trainer = pl.Trainer(max_epochs=1,default_root_dir='./batch3_logs')\n",
    "\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60616d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_classes = ['Clean','Dirty']\n",
    "\n",
    "# def imshow(imgnumpy: np.ndarray, label, denormalize=False):\n",
    "#     plt.imshow(tensor_to_imgnumpy_simple(imgnumpy))\n",
    "#     plt.title(dataset_classes[label])\n",
    "    \n",
    "# loader = DataLoader(model.dataset_test, batch_size=1, shuffle=True)\n",
    "\n",
    "# plt.figure(figsize=(20, 8))\n",
    "# for idx,(img,label) in enumerate(loader):\n",
    "#     plt.subplot(4,10,idx+1)\n",
    "#     imshow(img[0],label,denormalize=True)\n",
    "    \n",
    "#     # inference\n",
    "#     try:\n",
    "#         pred = model.forward(img.cuda())\n",
    "#     except Exception as e:\n",
    "#         pred =  model.forward(img)\n",
    "#         print(e)\n",
    "\n",
    "#     title_dataset = dataset_classes[label]\n",
    "#     title_pred = dataset_classes[pred.argmax()]\n",
    "#     plt.title(f\"{title_dataset}({title_pred})\",color=(\"green\" if title_dataset==title_pred else \"red\"))\n",
    "    \n",
    "#     if idx == 40-1:\n",
    "#         break\n",
    "        \n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22abc233",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b4ae9074d5523ac3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b4ae9074d5523ac3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir custom_logs/ --port=6013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3f2b50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 88\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: D:\\Documents\\Rino School Work\\School Work\\University\\Thesis\\Project\\lightning_logs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5d55f484204b0a92872eeb0b00d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(88)\n",
    "path = \"./checkpoint_test/checkpoints/epoch=49-step=1350.ckpt\"\n",
    "model = FinetunedModel.load_from_checkpoint(checkpoint_path=path, strict=False)\n",
    "\n",
    "model.current_epoch\n",
    "\n",
    "\n",
    "# print(model.learning_rate)\n",
    "# prints the learning_rate you used in this checkpoint\n",
    "                                                                    \n",
    "# model.eval()\n",
    "# y_hat = model(x)\n",
    "\n",
    "# model = FinetunedModel()\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "trainer.test(model)\n",
    "# checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "# print(checkpoint[\"state_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889015b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./custom_logs/lightning_logs/version_2//checkpoints/epoch=0-step=11.ckpt\"\n",
    "# model = FinetunedModel.load_from_checkpoint(checkpoint_path=path)\n",
    "\n",
    "# model = FinetunedModel()\n",
    "# trainer = pl.Trainer()\n",
    "\n",
    "\n",
    "# trainer.fit(model, ckpt_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3bb45bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch not compiled with CUDA enabled\n",
      "17\n",
      "Torch not compiled with CUDA enabled\n",
      "16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\cuda\\__init__.py:210\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(img\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 15\u001b[0m     pred \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem())    \n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mFinetunedModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m x \u001b[38;5;241m=\u001b[39m fix_resolution(x, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    124\u001b[0m warn_normalization(x)\n\u001b[1;32m--> 126\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapply_sigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_sigmoid:\n\u001b[0;32m    129\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchvision\\models\\resnet.py:283\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchvision\\models\\resnet.py:271\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    268\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    269\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchvision\\models\\resnet.py:152\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    149\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_classes = ['Clean','Dirty']\n",
    "    \n",
    "loader = DataLoader(model.dataset_test, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "targets = []\n",
    "preds = []\n",
    "\n",
    "for idx,(img,label) in enumerate(loader):\n",
    "    targets.append(label.item())\n",
    "    \n",
    "    try:\n",
    "        pred = model.forward(img.cuda())\n",
    "    except Exception as e:\n",
    "        pred =  model.forward(img)\n",
    "        print(e)\n",
    "\n",
    "    preds.append(pred.argmax().item())\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics import AUC\n",
    "\n",
    "targets_torch = torch.tensor(targets)\n",
    "preds_torch = torch.tensor(preds)\n",
    "\n",
    "print(preds)\n",
    "print(targets)\n",
    "\n",
    "confmat = ConfusionMatrix(num_classes=2)\n",
    "print(\"Confusion Matrix: \\nClean - Dirty\")\n",
    "print(confmat(preds_torch, targets_torch))\n",
    "\n",
    "auc = AUC(reorder=True)\n",
    "auc.update(preds_torch, targets_torch)\n",
    "print(\"AUC score: \")\n",
    "print(auc.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f912bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(preds_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a24819fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(17), tensor(16), tensor(16), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(14), tensor(17), tensor(16)]\n",
      "[1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Confusion Matrix: \n",
      "Clean - Dirty\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `preds` and `target` should have the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m confmat \u001b[38;5;241m=\u001b[39m ConfusionMatrix(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClean - Dirty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_torch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m auc \u001b[38;5;241m=\u001b[39m AUC(reorder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m auc\u001b[38;5;241m.\u001b[39mupdate(preds_torch, targets_torch)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\metric.py:236\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\metric.py:300\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    301\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\metric.py:380\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m--> 380\u001b[0m     update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\classification\\confusion_matrix.py:124\u001b[0m, in \u001b[0;36mConfusionMatrix.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m        preds: Predictions from model\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m        target: Ground truth values\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     confmat \u001b[38;5;241m=\u001b[39m \u001b[43m_confusion_matrix_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultilabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfmat \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m confmat\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\functional\\classification\\confusion_matrix.py:38\u001b[0m, in \u001b[0;36m_confusion_matrix_update\u001b[1;34m(preds, target, num_classes, threshold, multilabel)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_confusion_matrix_update\u001b[39m(\n\u001b[0;32m     26\u001b[0m     preds: Tensor, target: Tensor, num_classes: \u001b[38;5;28mint\u001b[39m, threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, multilabel: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     27\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"Updates and returns confusion matrix (without any normalization) based on the mode of the input.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m        multilabel: determines if data is multilabel or not.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     preds, target, mode \u001b[38;5;241m=\u001b[39m \u001b[43m_input_format_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (DataType\u001b[38;5;241m.\u001b[39mBINARY, DataType\u001b[38;5;241m.\u001b[39mMULTILABEL):\n\u001b[0;32m     40\u001b[0m         preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:408\u001b[0m, in \u001b[0;36m_input_format_classification\u001b[1;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m    406\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m--> 408\u001b[0m case \u001b[38;5;241m=\u001b[39m \u001b[43m_check_classification_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m (DataType\u001b[38;5;241m.\u001b[39mBINARY, DataType\u001b[38;5;241m.\u001b[39mMULTILABEL) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m top_k:\n\u001b[0;32m    419\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mint()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:266\u001b[0m, in \u001b[0;36m_check_classification_inputs\u001b[1;34m(preds, target, threshold, num_classes, multiclass, top_k, ignore_index)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m\"\"\"Performs error checking on inputs for classification.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03mThis ensures that preds and target take one of the shape/type combinations that are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m        'multi-dim multi-class'\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Basic validation (that does not need case/type information)\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43m_basic_input_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulticlass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Check that shape/types fall into one of the cases\u001b[39;00m\n\u001b[0;32m    269\u001b[0m case, implied_classes \u001b[38;5;241m=\u001b[39m _check_shape_and_type_consistency(preds, target)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3\\lib\\site-packages\\torchmetrics\\utilities\\checks.py:57\u001b[0m, in \u001b[0;36m_basic_input_validation\u001b[1;34m(preds, target, threshold, multiclass, ignore_index)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `preds` are integers, they have to be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `preds` and `target` should have the same first dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiclass \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you set `multiclass=False`, then `target` should not exceed 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The `preds` and `target` should have the same first dimension."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2294042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.fc.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265b617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
